{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "# How to use the Radiant MLHub API to browse and download the LandCoverNet dataset\n",
    "\n",
    "This Jupyter notebook, which you may copy and adapt for any use, shows basic examples of how to use the API to download labels and source imagery for the LandCoverNet dataset. Full documentation for the API is available at [docs.mlhub.earth](http://docs.mlhub.earth).\n",
    "\n",
    "We'll show you how to set up your authorization, list collection properties, and retrieve the items (the data contained within them) from those collections.\n",
    "\n",
    "Each item in our collection is explained in json format compliant with STAC label extension definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "Alemohammad S.H., Ballantyne A., Bromberg Gaber Y., Booth K., Nakanuku-Diggs L., & Miglarese A.H. (2020) \"LandCoverNet: A Global Land Cover Classification Training Dataset\", Version 1.0, Radiant MLHub. \\[Date Accessed\\] [https://doi.org/10.34911/rdnt.d2ce8i](https://doi.org/10.34911/rdnt.d2ce8i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "This notebook utilizes the [`radiant-mlhub` Python client](https://pypi.org/project/radiant-mlhub/) for interacting with the API. If you are running this notebooks using Binder, then this dependency has already been installed. If you are running this notebook locally, you will need to install this yourself.\n",
    "\n",
    "See the official [`radiant-mlhub` docs](https://radiant-mlhub.readthedocs.io/) for more documentation of the full functionality of that library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "### Create an API Key\n",
    "\n",
    "Access to the Radiant MLHub API requires an API key. To get your API key, go to [dashboard.mlhub.earth](https://dashboard.mlhub.earth). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. In the **API Keys** tab, you'll be able to create API key(s), which you will need. *Do not share* your API key with others: your usage may be limited and sharing your API key is a security risk.\n",
    "\n",
    "### Configure the Client\n",
    "\n",
    "Once you have your API key, you need to configure the `radiant_mlhub` library to use that key. There are a number of ways to configure this (see the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html) for details). \n",
    "\n",
    "For these examples, we will set the `MLHUB_API_KEY` environment variable. Run the cell below to save your API key as an environment variable that the client library will recognize.\n",
    "\n",
    "*If you are running this notebook locally and have configured a profile as described in the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html), then you do not need to execute this cell.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MLHUB_API_KEY'] = 'PASTE_YOUR_API_KEY_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import re\n",
    "from pathlib import Path\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from radiant_mlhub import client, get_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing Collection Properties\n",
    "\n",
    "The following cell makes a request to the API for the properties for the LandCoverNet labels collection and prints out a few important properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = 'ref_landcovernet_v1_labels'\n",
    "\n",
    "collection = client.get_collection(collection_id)\n",
    "print(f'Description: {collection[\"description\"]}')\n",
    "print(f'License: {collection[\"license\"]}')\n",
    "print(f'DOI: {collection[\"sci:doi\"]}')\n",
    "print(f'Citation: {collection[\"sci:citation\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Possible Land Cover Labels\n",
    "\n",
    "Each label item within the collection has a property which lists all of the possible land cover types and which ones are present in each label item. The code below prints out which land cover types are present in the dataset and we will reference these later in the notebook when we filter downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = client.list_collection_items(collection_id, limit=1)\n",
    "\n",
    "first_item = next(items)\n",
    "\n",
    "label_classes = first_item['properties']['label:classes']\n",
    "for label_class in label_classes:\n",
    "    print(f'Classes for {label_class[\"name\"]}')\n",
    "    for c in sorted(label_class['classes']):\n",
    "        print(f'- {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Assets\n",
    "\n",
    "> **NOTE:** If you are running these notebooks using Binder these resources will be downloaded to the remote file system that the notebooks are running on and **not to your local file system.** If you want to download the files to your machine, you will need to clone the repo and run the notebook locally.\n",
    "\n",
    "### Create Download Helpers\n",
    "\n",
    "The cell below creates 3 helper functions that we will use to select items from a collection and download the associated assets (source imagery or labels).\n",
    "\n",
    "* **`get_items`**\n",
    "\n",
    "    This is a [Python generator](https://realpython.com/introduction-to-python-generators/) that yields items from the given collection that match the criteria we give it. For instance, the following code will yield up to 10 items from the BigEarthNet labels collection that contain *either the `'Coniferous forest'` or the `'Rice fields'` labels*:\n",
    "    ```python\n",
    "    get_items('bigearthnet_v1_labels', classes=['Coniferous forest', 'Rice fields'], max_items=10)\n",
    "    ```\n",
    "\n",
    "* **`download`** \n",
    "\n",
    "    This function takes an item dictionary and an asset key and downloads the given asset. By default, the asset is downloaded to the current working directory, but this can be changed using the `output_dir` argument.\n",
    "\n",
    "* **`filter_item`** \n",
    "\n",
    "    This is a helper function used by the `get_items` function to filter items returned by `client.list_collection_items`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_pattern = re.compile(r'^/mlhub/v1/collections/(\\w+)/items/(\\w+)$')\n",
    "\n",
    "\n",
    "def filter_item(item, classes=None, cloud_and_shadow=None, seasonal_snow=None):\n",
    "    \"\"\"Function to be used as an argument to Python's built-in filter function that filters out any items that \n",
    "    do not match the given classes, cloud_and_shadow, and/or seasonal_snow values.\n",
    "    \n",
    "    If any of these filter arguments are set to None, they will be ignored. For instance, using \n",
    "    filter_item(item, cloud_and_shadow=True) will only return items where item['properties']['cloud_and_shadow'] == 'true', \n",
    "    and will not filter based on classes/labels, or seasonal_snow.\n",
    "    \"\"\"\n",
    "    # Match classes, if provided\n",
    "    \n",
    "    item_labels = item['properties'].get('labels', [])\n",
    "    if classes is not None and not any(label in classes for label in item_labels):\n",
    "        return False\n",
    "    \n",
    "    # Match cloud_and_shadow, if provided\n",
    "    item_cloud_and_shadow = item['properties'].get('cloud_and_shadow', 'false') == 'true'\n",
    "    if cloud_and_shadow is not None and item_cloud_and_shadow != cloud_and_shadow:\n",
    "        return False\n",
    "    \n",
    "    # Match seasonal_snow, if provided\n",
    "    item_seasonal_snow = item['properties'].get('seasonal_snow', 'false') == 'true'\n",
    "    if seasonal_snow is not None and item_seasonal_snow != seasonal_snow:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def get_items(collection_id, classes=None, cloud_and_shadow=None, seasonal_snow=None, max_items=1):\n",
    "    \"\"\"Generator that yields up to max_items items that match the given classes, cloud_and_shadow, and seasonal_snow \n",
    "    values. Setting one of these filter arguments to None will cause that filter to be ignored (e.g. classes=None \n",
    "    means that items will not be filtered by class/label).\n",
    "    \"\"\"\n",
    "    filter_fn = partial(\n",
    "        filter_item, \n",
    "        classes=classes, \n",
    "        cloud_and_shadow=cloud_and_shadow, \n",
    "        seasonal_snow=seasonal_snow\n",
    "    )\n",
    "    filtered = filter(\n",
    "        filter_fn, \n",
    "\n",
    "        # Note that we set the limit to None here because we want to limit based on our own filters. It is not \n",
    "        #  recommended to use limit=None for the client.list_collection_items method without implementing your \n",
    "        #  own limits because the bigearthnet_v1_labels collection contains hundreds of thousands of items and \n",
    "        #  looping over these items without limit may take a very long time.\n",
    "        client.list_collection_items(collection_id, limit=None)\n",
    "    )\n",
    "    yield from it.islice(filtered, max_items)\n",
    "    \n",
    "\n",
    "def download(item, asset_key, output_dir='./data'):\n",
    "    \"\"\"Downloads the given item asset by looking up that asset and then following the \"href\" URL.\"\"\"\n",
    "\n",
    "    # Try to get the given asset and return None if it does not exist\n",
    "    asset = item.get('assets', {}).get(asset_key)\n",
    "    if asset is None:\n",
    "        print(f'Asset \"{asset_key}\" does not exist in this item')\n",
    "        return None\n",
    "    \n",
    "    # Try to get the download URL from the asset and return None if it does not exist\n",
    "    download_url = asset.get('href')\n",
    "    if download_url is None:\n",
    "        print(f'Asset {asset_key} does not have an \"href\" property, cannot download.')\n",
    "        return None\n",
    "    \n",
    "    session = get_session()\n",
    "    r = session.get(download_url, allow_redirects=True, stream=True)\n",
    "    \n",
    "    filename = urllib.parse.urlsplit(r.url).path.split('/')[-1]\n",
    "    output_path = Path(output_dir) / filename\n",
    "\n",
    "    \n",
    "    with output_path.open('wb') as dst:\n",
    "        for chunk in r.iter_content(chunk_size=512 * 1024):\n",
    "            if chunk:\n",
    "                dst.write(chunk)\n",
    "    \n",
    "\n",
    "def download_labels_and_source(item, assets=None, output_dir='./data'):\n",
    "    \"\"\"Downloads all label and source imagery assets associated with a label item that match the given asset types.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Follow all source links and add all assets from those\n",
    "    def _get_download_args(link):\n",
    "        # Get the item ID (last part of the link path)\n",
    "        source_item_path = urllib.parse.urlsplit(link['href']).path\n",
    "        source_item_collection, source_item_id = items_pattern.fullmatch(source_item_path).groups()\n",
    "        source_item = client.get_collection_item(source_item_collection, source_item_id)\n",
    "\n",
    "        source_download_dir = download_dir / 'source'\n",
    "        source_download_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        matching_source_assets = [\n",
    "            asset \n",
    "            for asset in source_item.get('assets', {}) \n",
    "            if assets is None or asset in assets\n",
    "        ] \n",
    "        return [\n",
    "            (source_item, asset, source_download_dir) \n",
    "            for asset in matching_source_assets\n",
    "        ]\n",
    "\n",
    "    \n",
    "    download_args = []\n",
    "    \n",
    "    download_dir = Path(output_dir) / item['id']\n",
    "    download_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    labels_download_dir = download_dir / 'labels'\n",
    "    labels_download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Download the labels assets\n",
    "    matching_assets = [\n",
    "        asset \n",
    "        for asset in item.get('assets', {}) \n",
    "        if assets is None or asset in assets\n",
    "    ]\n",
    "\n",
    "    for asset in matching_assets:\n",
    "        download_args.append((item, asset, labels_download_dir))\n",
    "        \n",
    "    source_links = [link for link in item['links'] if link['rel'] == 'source']\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        for argument_batch in executor.map(_get_download_args, source_links):\n",
    "            download_args += argument_batch\n",
    "        \n",
    "    print(f'Downloading {len(download_args)} assets...')\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        with tqdm(total=len(download_args)) as pbar:\n",
    "            for _ in executor.map(lambda triplet: download(*triplet), download_args):\n",
    "                pbar.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Assets for 1 Item\n",
    "\n",
    "The following cell below will navigate and API and collect all the download links for labels and source imagery assets. \n",
    "\n",
    "In this case we specified the `max_items` argument to the `get_items` function, which limits the number of label items fetched to just 1. We also pass a list of `assets` to the `download_labels_and_source` function, which limits the types of assets downloaded to only those included in the list. We limit the results in these two ways because there a nearly 2,000 label items and over 150,000 source items in the LandCoverNet collections, and each source item contains at least 13 items representing the various Sentinel 2 bands. Attempting to download all items or all assets for even a few items can take a very long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items = get_items(\n",
    "    collection_id,\n",
    "    max_items=1\n",
    ")\n",
    "for item in items:\n",
    "    download_labels_and_source(item, assets=['labels', 'B02', 'B03', 'B04'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering on Land Cover Type\n",
    "\n",
    "We can specify which land cover types we want to download by adding the \"classes\" argument. This argument accepts an array of land cover types and only label items which contain one or more of the classes specified will be downloaded. The possible land cover types can be found in the \"Finding Possible Land Cover Labels\" cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = get_items(\n",
    "    collection_id,\n",
    "    classes=['Woody Vegetation'],\n",
    "    max_items=1,\n",
    ")\n",
    "for item in items:\n",
    "    download_labels_and_source(item, assets=['labels', 'B02', 'B03', 'B04'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download All Assets\n",
    "\n",
    "Looping through all items and downloading the associated assets may be *very* time-consuming for larger datasets like LandCoverNet. Instead, MLHub provides TAR archives of all collections that can be downloaded using the `/archive/{collection_id}` endpoint. \n",
    "\n",
    "The following cell uses the `client.download_archive` function to download the `ref_landcovernet_v1_labels` archive to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.download_archive(collection_id, output_dir='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
